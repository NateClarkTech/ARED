{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a0674bd9-2de8-4204-be41-c8e249004033",
   "metadata": {
    "id": "a0674bd9-2de8-4204-be41-c8e249004033"
   },
   "source": [
    "# Circular Buffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3c85546b-1bd5-47ec-9cd4-6a0e8b7467e2",
   "metadata": {
    "id": "3c85546b-1bd5-47ec-9cd4-6a0e8b7467e2"
   },
   "outputs": [],
   "source": [
    "class Circular_Buffer:\n",
    "    def __init__(self, size):\n",
    "        self.buffer = [None] * size\n",
    "        self.size = size\n",
    "        self.start = 0  # Points to the oldest element\n",
    "        self.count = 0  # Number of elements currently in the buffer\n",
    "\n",
    "    def append(self, value):\n",
    "        index = (self.start + self.count) % self.size\n",
    "        if self.count < self.size:\n",
    "            self.buffer[index] = value\n",
    "            self.count += 1\n",
    "            return None  # No value was overwritten\n",
    "        else:\n",
    "            overwritten = self.buffer[self.start]\n",
    "            self.buffer[self.start] = value\n",
    "            self.start = (self.start + 1) % self.size\n",
    "            return overwritten\n",
    "\n",
    "    def set_at(self, index, value):\n",
    "        \"\"\"Set value at a relative index within the buffer (0 = oldest).\"\"\"\n",
    "        if index < 0 or index >= self.count:\n",
    "            raise IndexError(f\"Index out of bounds in circular buffer {index} {value}\")\n",
    "        real_index = (self.start + index) % self.size\n",
    "        self.buffer[real_index] = value\n",
    "\n",
    "    def get(self, idx):\n",
    "      \"\"\"Return the element at the given relative index (0 is the oldest element).\"\"\"\n",
    "      if idx < 0 or idx >= self.count:\n",
    "          raise IndexError(f\"Index out of bounds in circular buffer. {idx}\")\n",
    "      return self.buffer[(self.start + idx) % self.size]\n",
    "\n",
    "    def print_array(self):\n",
    "        \"\"\"Print the contents of the buffer in order from oldest to newest.\"\"\"\n",
    "        elements = [self.buffer[(self.start + i) % self.size] for i in range(self.count)]\n",
    "        print(elements)\n",
    "\n",
    "    def get_array(self):\n",
    "        return [self.buffer[(self.start + i) % self.size] for i in range(self.count)]\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return f\"Circular_Buffer({self.get()})\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35f032ec-68b5-4322-9fbe-f3f04123c610",
   "metadata": {
    "id": "35f032ec-68b5-4322-9fbe-f3f04123c610"
   },
   "source": [
    "# Data Window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ec6fc352-1636-44a3-8048-8b0e6c9ce593",
   "metadata": {
    "id": "ec6fc352-1636-44a3-8048-8b0e6c9ce593"
   },
   "outputs": [],
   "source": [
    "class Data_Window:\n",
    "    def __init__(self, data_window_size = 1000):\n",
    "        self.abs_idx_max = -1 # abs_idx_max is the absolute index most recent point inserted\n",
    "        self.abs_idx_min = 0\n",
    "        self.data_window_size = data_window_size\n",
    "        self.assigned_cluster_id_window = Circular_Buffer(data_window_size)\n",
    "        self.is_point_labeled_window = Circular_Buffer(data_window_size)\n",
    "        self.data_in_window = Circular_Buffer(data_window_size)\n",
    "        self.last_removed_cluster_id = None # Cluster ID of most recently forgotten point, the abs_idx of that point is abs_idx_min - 1\n",
    "\n",
    "    def insert_data(self, data_point):\n",
    "        self.data_in_window.append(data_point)\n",
    "        self.abs_idx_max += 1\n",
    "        self.abs_idx_min = max(0, self.abs_idx_max - self.data_window_size + 1)\n",
    "\n",
    "        self.last_removed_cluster_id = self.assigned_cluster_id_window.append(None)\n",
    "\n",
    "        self.is_point_labeled_window.append(False)\n",
    "\n",
    "    # def insert_cluster_id(self, cluster_id):\n",
    "    #     self.last_removed_cluster_id = self.assigned_cluster_id_window.append(cluster_id)\n",
    "\n",
    "    def get_data_point(self, abs_index):\n",
    "        if not (self.abs_idx_min <= abs_index <= self.abs_idx_max):\n",
    "            raise IndexError(f\"abs_index {abs_index} is out of the window range \"\n",
    "                             f\"[{self.abs_idx_min}, {self.abs_idx_max})\")\n",
    "\n",
    "        dw_index = abs_index - self.abs_idx_min\n",
    "        return self.data_in_window.get(dw_index)\n",
    "\n",
    "    def update_cluster_id_at(self, abs_index, new_id):\n",
    "        if not (self.abs_idx_min <= abs_index <= self.abs_idx_max):\n",
    "            raise IndexError(f\"abs_index {abs_index} is out of the window range \"\n",
    "                             f\"[{self.abs_idx_min}, {self.abs_idx_max})\")\n",
    "\n",
    "        dw_index = abs_index - self.abs_idx_min\n",
    "        self.assigned_cluster_id_window.set_at(dw_index, new_id)\n",
    "\n",
    "    def updated_labeled_window(self, abs_index):\n",
    "      if not (self.abs_idx_min <= abs_index <= self.abs_idx_max):\n",
    "            raise IndexError(f\"abs_index {abs_index} is out of the window range \"\n",
    "                             f\"[{self.abs_idx_min}, {self.abs_idx_max})\")\n",
    "\n",
    "      dw_index = abs_index - self.abs_idx_min\n",
    "      self.is_point_labeled_window.set_at(dw_index, True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c16d47c2-d78e-40c0-b965-4675d80441fc",
   "metadata": {
    "id": "c16d47c2-d78e-40c0-b965-4675d80441fc"
   },
   "source": [
    "# Labeled Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c3897fbd-08c3-4626-989d-2b23c15f13f7",
   "metadata": {
    "id": "c3897fbd-08c3-4626-989d-2b23c15f13f7"
   },
   "outputs": [],
   "source": [
    "class Labeled_Data:\n",
    "  def __init__(self):\n",
    "    self.abs_idx_array = []\n",
    "    self.data_array = []\n",
    "    self.cluster_id_array = []\n",
    "    self.label_array = []\n",
    "    self.relevance_array = []\n",
    "\n",
    "  def add_point(self, abs_idx, data_point, cluster_id, label, relevance):\n",
    "    self.abs_idx_array.append(abs_idx)\n",
    "    self.data_array.append(data_point)\n",
    "    self.cluster_id_array.append(cluster_id)\n",
    "    self.label_array.append(label)\n",
    "    self.relevance_array.append(relevance)\n",
    "\n",
    "  def get_data(self, abs_idx):\n",
    "    return self.data_array[self.abs_idx_array.index(abs_idx)]\n",
    "\n",
    "  def get_ld_index(self, abs_idx):\n",
    "    return self.abs_idx_array.index(abs_idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2485351-0797-47e7-adc3-dc97b3fdf405",
   "metadata": {
    "id": "c2485351-0797-47e7-adc3-dc97b3fdf405"
   },
   "source": [
    "# Subspace Partition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0b51dbe5-38fa-436e-aa1f-1e5d7749b643",
   "metadata": {
    "id": "0b51dbe5-38fa-436e-aa1f-1e5d7749b643"
   },
   "outputs": [],
   "source": [
    "class Subspace_Partition:\n",
    "    def __init__(self):        #                                                                   (l_pts)          (o_pts)\n",
    "        self.cluster_list = [] # cluster is expected to be in the format of [label, relevance, [abs_idx_l_pt], [abs_idx_o_pt], diameter]\n",
    "        self.set_of_known_labels = set()\n",
    "        # cluster id is the cluster's index in cluster_list\n",
    "\n",
    "    def create_new_cluster(self, label, relevance, l_pts, o_pts, labeled_data):\n",
    "      self.set_of_known_labels.add(label)\n",
    "      self.cluster_list.append(Cluster(label, relevance, l_pts, o_pts, labeled_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "lrSs46rol6KE",
   "metadata": {
    "id": "lrSs46rol6KE"
   },
   "source": [
    "# Cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "WaB_Bet-l5vR",
   "metadata": {
    "id": "WaB_Bet-l5vR"
   },
   "outputs": [],
   "source": [
    "class Cluster:\n",
    "  def __init__(self, label, relevance, l_pts, o_pts, labeled_data):\n",
    "    self.label = label\n",
    "    self.relevance = relevance\n",
    "    self.l_pts = l_pts\n",
    "    self.o_pts = o_pts\n",
    "    self.diameter = 0\n",
    "    # cluster id is this cluster's position in Subspace_Partition.cluster_list\n",
    "\n",
    "    if len(l_pts) > 1:\n",
    "      self.update_diameter(labeled_data)\n",
    "\n",
    "  def add_l_pt(self, abs_idx, labeled_data):\n",
    "    self.l_pts.append(abs_idx)\n",
    "    self.update_diameter(labeled_data)\n",
    "\n",
    "  def add_o_pt(self, abs_idx):\n",
    "    self.o_pts.append(abs_idx)\n",
    "\n",
    "  def update_diameter(self, labeled_data):\n",
    "    largest_distance = 0\n",
    "    for i in range(len(self.l_pts)):\n",
    "      for j in range(i):\n",
    "        data_l_pt_i = labeled_data.get_data(self.l_pts[i])\n",
    "        data_l_pt_j = labeled_data.get_data(self.l_pts[j])\n",
    "        distance = np.linalg.norm(data_l_pt_i - data_l_pt_j)\n",
    "        if largest_distance < distance:\n",
    "            largest_distance = distance\n",
    "    self.diameter = largest_distance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "JXwQUXZfsRf1",
   "metadata": {
    "id": "JXwQUXZfsRf1"
   },
   "source": [
    "# Oracle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "DeVRTkZwsRMM",
   "metadata": {
    "id": "DeVRTkZwsRMM"
   },
   "outputs": [],
   "source": [
    "class Oracle:\n",
    "  def __init__(self, X, y):\n",
    "    self.X = X #[[data]]\n",
    "    self.y = y #[[label, relevant]]\n",
    "\n",
    "  def answer_query(self, abs_index):\n",
    "    label = self.y[abs_index][0]\n",
    "    relevance = self.y[abs_index][1]\n",
    "    return (label, relevance)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "PU-2rDuzuRk4",
   "metadata": {
    "id": "PU-2rDuzuRk4"
   },
   "source": [
    "# Data Stream"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "eVYLY_kkuR2V",
   "metadata": {
    "id": "eVYLY_kkuR2V"
   },
   "outputs": [],
   "source": [
    "class Data_Stream:\n",
    "  def __init__(self, X, y):\n",
    "    self.X = X #[[data]]\n",
    "    self.y = y #[[label, relevant]]\n",
    "    self.stream_counter = 0\n",
    "\n",
    "  def stream_new_data_point(self):\n",
    "    data_point = self.X[self.stream_counter]\n",
    "    self.stream_counter += 1\n",
    "    return data_point\n",
    "\n",
    "  def get_remaining_num_points(self):\n",
    "    return len(self.X) - self.stream_counter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2c52f0f-3957-46e4-a167-8b68f9aa9458",
   "metadata": {
    "id": "d2c52f0f-3957-46e4-a167-8b68f9aa9458"
   },
   "source": [
    "# ARED"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3ef0e908-94e9-431e-9ff4-48792a45256c",
   "metadata": {
    "id": "86978c85-7fd3-4396-81e4-5724697db693"
   },
   "outputs": [],
   "source": [
    "## import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "class ARED:\n",
    "    def __init__(self, oracle, kappa=1.0, data_window_size=1000, verbose = False):\n",
    "        self.kappa = kappa\n",
    "        self.data_window = Data_Window(data_window_size)\n",
    "        self.labeled_data = Labeled_Data()\n",
    "        self.subspace_partition = Subspace_Partition()\n",
    "        self.oracle = oracle\n",
    "        self.verbose = verbose\n",
    "\n",
    "\n",
    "    def process_first_point(self, data_point):\n",
    "\n",
    "      # Insert data point into data_window\n",
    "      self.data_window.insert_data(data_point)\n",
    "      data_point_abs_idx = self.data_window.abs_idx_max\n",
    "\n",
    "      # START QUERY\n",
    "      label, relevance = self.query(data_point_abs_idx)\n",
    "      # END QUERY\n",
    "\n",
    "      cluster_id = 0\n",
    "\n",
    "      # Update data_window.assigned_cluster_id_window\n",
    "      self.data_window.update_cluster_id_at(0, 0)\n",
    "\n",
    "      # Create new cluster\n",
    "      self.labeled_data.add_point(data_point_abs_idx, data_point, cluster_id, label, relevance) #cluster_id = 0\n",
    "      self.subspace_partition.create_new_cluster(label, relevance, [data_point_abs_idx], [], self.labeled_data)\n",
    "\n",
    "      if self.verbose:\n",
    "        print(\"new cluster:\", 0, [0])\n",
    "\n",
    "\n",
    "    def determine_comparison_cluster(self, data_point):\n",
    "\n",
    "      shortest_distance = np.inf\n",
    "      closest = None\n",
    "\n",
    "      for i, cluster in enumerate(self.subspace_partition.cluster_list):\n",
    "        for abs_l_pt_index in cluster.l_pts:\n",
    "          l_pt_data = self.labeled_data.get_data(abs_l_pt_index)\n",
    "          distance = np.linalg.norm(l_pt_data - data_point)\n",
    "\n",
    "          if distance < shortest_distance:\n",
    "            shortest_distance = distance\n",
    "            closest = (i, distance)\n",
    "\n",
    "      return closest # (cluster_id, distance)\n",
    "\n",
    "\n",
    "    def anomalous(self, data_point, cluster_id, distance):\n",
    "        cluster = self.subspace_partition.cluster_list[cluster_id]\n",
    "\n",
    "        if len(cluster.l_pts) <= 1:\n",
    "          return True  # Can't define a diameter with fewer than 2 points\n",
    "\n",
    "        # Point is anomalous if its distance is greater than the cluster's diameter\n",
    "        return distance * self.kappa > cluster.diameter\n",
    "\n",
    "\n",
    "    def query(self, abs_data_index):\n",
    "        self.data_window.updated_labeled_window(abs_data_index)\n",
    "        # return (label, relevance) from oracle\n",
    "        return self.oracle.answer_query(abs_data_index)\n",
    "\n",
    "\n",
    "    # ran when we add a new o_pt to a cluster\n",
    "    def add_o_pt(self, abs_idx, cluster_id):\n",
    "\n",
    "      if self.verbose:\n",
    "        print(\"add_o_pt:\", abs_idx, cluster_id)\n",
    "\n",
    "      cluster = self.subspace_partition.cluster_list[cluster_id]\n",
    "      cluster.add_o_pt(abs_idx)\n",
    "\n",
    "      # update data_window.assigned_cluster_id_window\n",
    "      self.data_window.update_cluster_id_at(abs_idx, cluster_id)\n",
    "\n",
    "\n",
    "    # ran when we add a new labeled data point to a known cluster\n",
    "    def add_l_pt(self, abs_idx, data_point, cluster_id):\n",
    "\n",
    "      if self.verbose:\n",
    "        print(\"add_l_pt:\", abs_idx, cluster_id)\n",
    "\n",
    "      # update cluster in subspace parition\n",
    "      cluster = self.subspace_partition.cluster_list[cluster_id]\n",
    "\n",
    "      # get label and relevance\n",
    "      label = cluster.label\n",
    "      relevance = cluster.relevance\n",
    "\n",
    "      # update data_window.assigned_cluster_id_window\n",
    "      self.data_window.update_cluster_id_at(abs_idx, cluster_id)\n",
    "\n",
    "      # update labeled_data to have the new point\n",
    "      self.labeled_data.add_point(abs_idx, data_point, cluster_id, label, relevance)\n",
    "\n",
    "      # add point to cluster, so diameter gets updated properly\n",
    "      cluster.add_l_pt(abs_idx, self.labeled_data)\n",
    "\n",
    "\n",
    "    def split(self, data_point, data_point_idx, new_cluster_label, relevance, old_cluster_id):\n",
    "\n",
    "      new_cluster_id = len(self.subspace_partition.cluster_list)\n",
    "      self.labeled_data.add_point(data_point_idx, data_point, new_cluster_id, new_cluster_label, relevance)\n",
    "      self.data_window.update_cluster_id_at(data_point_idx, new_cluster_id)\n",
    "      self.subspace_partition.create_new_cluster(new_cluster_label, relevance, [data_point_idx], [], self.labeled_data)\n",
    "\n",
    "      if self.verbose:\n",
    "        print(\"new cluster:\", new_cluster_id, [data_point_idx])\n",
    "\n",
    "      # array to hold o_pt indexes during the split process\n",
    "      new_cluster_o_pts_abs_inds = []\n",
    "      old_cluster_o_pts_abs_inds = []\n",
    "\n",
    "      # get o_pt indices\n",
    "      o_pts_abs_inds_to_split = self.subspace_partition.cluster_list[old_cluster_id].o_pts\n",
    "\n",
    "      if (len(o_pts_abs_inds_to_split) == 0):\n",
    "        #print(\"No o_pts to split\")\n",
    "        return\n",
    "\n",
    "      # get l_pt indices\n",
    "      l_pt_inds = self.subspace_partition.cluster_list[old_cluster_id].l_pts\n",
    "\n",
    "      # o_pt_index is an abs_idx\n",
    "      for o_pt_index in o_pts_abs_inds_to_split:\n",
    "          o_pt = self.data_window.get_data_point(o_pt_index)\n",
    "\n",
    "          # find the closest labeled point in the exisiting cluster\n",
    "          distance_to_existing = min([\n",
    "              np.linalg.norm(o_pt - self.labeled_data.get_data(l_pt_index))\n",
    "              for l_pt_index in l_pt_inds\n",
    "          ])\n",
    "\n",
    "          # get the distance to the labeled point in the new cluster\n",
    "          distance_to_new = np.linalg.norm(o_pt - data_point)\n",
    "\n",
    "          # put the o_pt in the closest cluster of the two\n",
    "          if distance_to_existing < distance_to_new:\n",
    "              old_cluster_o_pts_abs_inds.append(o_pt_index)\n",
    "          else:\n",
    "              print(distance_to_new, distance_to_existing, o_pt_index)\n",
    "              new_cluster_o_pts_abs_inds.append(o_pt_index)\n",
    "\n",
    "              # update the data window so the assigned_label_id_window is correct for window maintenance later\n",
    "              self.data_window.update_cluster_id_at(o_pt_index, new_cluster_id)\n",
    "\n",
    "      if self.verbose:\n",
    "        print(\"Split :\")\n",
    "        print(\"old_cluster_id w/ o_pts:\", old_cluster_id, old_cluster_o_pts_abs_inds)\n",
    "        print(\"new_cluster_id w/ o_pts:\", new_cluster_id, new_cluster_o_pts_abs_inds)\n",
    "\n",
    "      # put the o_pts in their correct cluster\n",
    "      self.subspace_partition.cluster_list[new_cluster_id].o_pts = new_cluster_o_pts_abs_inds # update o_pts new_cluster\n",
    "      self.subspace_partition.cluster_list[old_cluster_id].o_pts = old_cluster_o_pts_abs_inds # update o_pts old_cluster\n",
    "\n",
    "    def relevance_processing(self, new_cluster_id):\n",
    "        pass\n",
    "\n",
    "    # Removing forgotten o_pts from the subspace partition\n",
    "    def subspace_partition_maintenance(self, forgotten_abs_idx, forgotten_point_cluster_id):\n",
    "\n",
    "      cluster = self.subspace_partition.cluster_list[forgotten_point_cluster_id]\n",
    "\n",
    "      if self.verbose:\n",
    "          print(forgotten_abs_idx, forgotten_point_cluster_id)\n",
    "\n",
    "      cluster.o_pts.remove(forgotten_abs_idx)\n",
    "\n",
    "    def showframe(self, abs_index):\n",
    "      im_data = self.data_window.get_data_point(abs_index)\n",
    "      im_data = im_data.reshape([128, 128, 3])\n",
    "      plt.imshow(im_data, cmap='gray')\n",
    "      plt.title(f\"Index: {abs_index}\")\n",
    "      plt.axis('off')\n",
    "\n",
    "    def process_point(self, data_point):\n",
    "\n",
    "      if self.verbose:\n",
    "          print(\"labeled id array:\", self.labeled_data.cluster_id_array)\n",
    "          print(\"labeled abs array:\", self.labeled_data.abs_idx_array)\n",
    "          print(\"data window assigned id:\", self.data_window.assigned_cluster_id_window.get_array())\n",
    "        \n",
    "\n",
    "      is_forgotten_point_labeled = self.data_window.is_point_labeled_window.get(0)\n",
    "\n",
    "      self.data_window.insert_data(data_point)\n",
    "      data_point_abs_idx = self.data_window.abs_idx_max\n",
    "\n",
    "      forgotten_abs_idx = self.data_window.abs_idx_min - 1\n",
    "      forgotten_pt_cluster_id = self.data_window.last_removed_cluster_id\n",
    "\n",
    "      # if forgotten_pt_cluster_id is NOT None (ie a point has been fogotten) do maintenance\n",
    "      if forgotten_pt_cluster_id != None and not is_forgotten_point_labeled:\n",
    "        self.subspace_partition_maintenance(forgotten_abs_idx, forgotten_pt_cluster_id)\n",
    "\n",
    "      # START DETERMINE COMPARISON CLUSTER\n",
    "\n",
    "      comp_cluster_id, distance = self.determine_comparison_cluster(data_point)\n",
    "\n",
    "      relevant = self.subspace_partition.cluster_list[comp_cluster_id].relevance\n",
    "\n",
    "      # END DETERMINE COMPARISON CLUSTER\n",
    "\n",
    "      # START NOT RELEVANT\n",
    "\n",
    "      if not relevant:\n",
    "        # START NOT ANOMALOUS\n",
    "        if not self.anomalous(data_point, comp_cluster_id, distance):\n",
    "\n",
    "          self.add_o_pt(data_point_abs_idx, comp_cluster_id)\n",
    "\n",
    "          return # Data point processed, END Function\n",
    "\n",
    "        # END NOT ANOMALOUS\n",
    "\n",
    "      #END NOT RELEVANT\n",
    "\n",
    "      # START QUERY\n",
    "      label, relevant = self.query(data_point_abs_idx)\n",
    "      #plt.figure()\n",
    "      #self.showframe(data_point_abs_idx)\n",
    "      # END QUERY\n",
    "\n",
    "      # START NOT NEW LABEL\n",
    "      label_is_new = label in self.subspace_partition.set_of_known_labels\n",
    "\n",
    "      if not label_is_new:\n",
    "        self.add_l_pt(data_point_abs_idx, data_point, comp_cluster_id)\n",
    "\n",
    "        return # Data point processed, END Function\n",
    "\n",
    "      # END NOT NEW LABEL\n",
    "\n",
    "      # START NEW LABEL\n",
    "      # create new cluster with the split o_pts\n",
    "\n",
    "      self.split(data_point, data_point_abs_idx, label, relevant, comp_cluster_id)\n",
    "\n",
    "      # END NEW LABEL\n",
    "\n",
    "      # START RELEVANCE PROCESSING\n",
    "      if relevant:\n",
    "        self.relevance_processing(len(self.subspace_partition.cluster_list) - 1)\n",
    "      # END RELEVANCE PROCESSING\n",
    "\n",
    "      # POINT PROCESSED"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "Nr7JXOdHiQmS",
   "metadata": {
    "id": "Nr7JXOdHiQmS"
   },
   "source": [
    "# ARED on Parking lot data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c099f5e",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8c099f5e",
    "outputId": "bf1e2023-8bda-41f8-efb5-b3481d159583"
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "import cv2\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "features_path = \"./features.pkl\"\n",
    "labels_path = \"./labels.csv\"\n",
    "\n",
    "with open(features_path, 'rb') as f:\n",
    "    features = pickle.load(f)  # Expecting a list or array of 128x128 flattened frames\n",
    "\n",
    "labels_df = pd.read_csv(labels_path)\n",
    "print(\"Features loaded successfully.\")\n",
    "print(\"Labels loaded successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "n3soAbw-JCjo",
   "metadata": {
    "id": "n3soAbw-JCjo"
   },
   "source": [
    "# Create Skewed MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "3604e191-78e4-4b27-acfc-c6d2244afd2f",
   "metadata": {
    "id": "3604e191-78e4-4b27-acfc-c6d2244afd2f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading MNIST from OpenML...\n",
      "Full MNIST loaded: 70000 samples\n",
      "Full MNIST (X, y) saved to mnist_full.pkl\n",
      "Creating skewed subset with sparsity [10000, 5000, 2000, 2000, 1000, 500, 300, 100, 50, 20] and max 20970 events...\n",
      "Warning: Not enough samples for digit 8, using all 6825\n",
      "Skewed dataset shape: (17795, 784)\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "from sklearn.datasets import fetch_openml\n",
    "\n",
    "# Create skewed subset\n",
    "def create_skewed_mnist(X, y, sparsity_levels, n_events):\n",
    "    np.random.seed(42)\n",
    "    digit_order = np.random.permutation(10)\n",
    "    indices = []\n",
    "    for digit, count in zip(digit_order, sparsity_levels):\n",
    "        digit_indices = np.where(y == str(digit))[0]\n",
    "        if len(digit_indices) >= count:\n",
    "            selected = np.random.choice(digit_indices, count, replace=False)\n",
    "            indices.extend(selected)\n",
    "        else:\n",
    "            print(f\"Warning: Not enough samples for digit {digit}, using all {len(digit_indices)}\")\n",
    "            indices.extend(digit_indices)\n",
    "    indices = np.array(indices)\n",
    "    np.random.shuffle(indices)\n",
    "    if len(indices) > n_events:\n",
    "        indices = indices[:n_events]\n",
    "    X_skewed = X[indices]\n",
    "    y_skewed = y[indices]\n",
    "    return X_skewed, y_skewed\n",
    "\n",
    "def load_and_skew_mnist(sparsity_levels, n_events, save_path=\"mnist_full.pkl\"):\n",
    "    \"\"\"\n",
    "    Loads MNIST, creates a skewed subset using create_skewed_mnist.\n",
    "    Also saves the full MNIST (X, y) to a pickle file.\n",
    "\n",
    "    Args:\n",
    "        sparsity_levels: list of 10 integers, number of samples per digit.\n",
    "        n_events: total number of samples to include in final skewed dataset.\n",
    "        save_path: path to store full MNIST data as pickle.\n",
    "\n",
    "    Returns:\n",
    "        X_skewed, y_skewed: filtered and shuffled MNIST subset.\n",
    "        X, y: full MNIST dataset.\n",
    "    \"\"\"\n",
    "    print(\"Loading MNIST from OpenML...\")\n",
    "    mnist = fetch_openml(\"mnist_784\", version=1, as_frame=False)\n",
    "    X, y = mnist.data, mnist.target  # y is a string array of digits\n",
    "\n",
    "    print(f\"Full MNIST loaded: {X.shape[0]} samples\")\n",
    "\n",
    "    # Save full dataset\n",
    "    with open(save_path, \"wb\") as f:\n",
    "        pickle.dump((X, y), f)\n",
    "    print(f\"Full MNIST (X, y) saved to {save_path}\")\n",
    "\n",
    "    # Create skewed subset\n",
    "    print(f\"Creating skewed subset with sparsity {sparsity_levels} and max {n_events} events...\")\n",
    "    X_skewed, y_skewed = create_skewed_mnist(X, y, sparsity_levels, n_events)\n",
    "\n",
    "    print(f\"Skewed dataset shape: {X_skewed.shape}\")\n",
    "    return X_skewed, y_skewed, X, y\n",
    "\n",
    "sparsity_levels = [10000, 5000, 2000, 2000, 1000, 500, 300, 100, 50, 20]\n",
    "n_events = sum(sparsity_levels)\n",
    "\n",
    "X_skewed, y_skewed, X_full, y_full = load_and_skew_mnist(sparsity_levels, n_events)\n",
    "\n",
    "def generate_is_relevant(y_skewed, relevant_digits={\"8\"}):\n",
    "    # mark some digits as relevant (e.g., rare or important events)\n",
    "    return np.array([label in relevant_digits for label in y_skewed], dtype=bool)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23984ca8-ab11-431a-b617-d1f3ed9d92f0",
   "metadata": {
    "id": "23984ca8-ab11-431a-b617-d1f3ed9d92f0"
   },
   "source": [
    "### ARED on Skewed MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "152fbcb2-7d28-4cf4-a841-2dbb96b1f91a",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 392
    },
    "id": "152fbcb2-7d28-4cf4-a841-2dbb96b1f91a",
    "outputId": "d39778c1-71ae-4300-d508-2759498034e0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Least common digits: ['3', '6'] (marked as relevant)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import fetch_openml\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "\n",
    "def generate_is_relevant(label_list, relevant_set):\n",
    "    return [label in relevant_set for label in label_list]\n",
    "\n",
    "def main():\n",
    "\n",
    "    #STEP 1 run last cell\n",
    "\n",
    "    # Step 2: Identify the 2 least common digits\n",
    "    digit_counts = Counter(y_skewed)\n",
    "    least_common_digits = [digit for digit, _ in digit_counts.most_common()[-2:]]\n",
    "\n",
    "    print(f\"Least common digits: {least_common_digits} (marked as relevant)\")\n",
    "\n",
    "    # Step 3: Generate relevance info\n",
    "    relevance_array = generate_is_relevant(y_skewed, set(least_common_digits))\n",
    "    y_w_rel = list(zip(y_skewed, relevance_array))\n",
    "\n",
    "    # Step 4: Initialize Oracle and ARED\n",
    "    data_stream = Data_Stream(X_skewed, y_w_rel)\n",
    "    oracle = Oracle(X_skewed, y_w_rel)\n",
    "    ared = ARED(oracle, 2, 1000, False)\n",
    "\n",
    "    # Step 5: Stream data\n",
    "    ared.process_first_point(data_stream.stream_new_data_point())\n",
    "    for _ in range(data_stream.get_remaining_num_points()): #data_stream.get_remaining_num_points()\n",
    "        ared.process_point(data_stream.stream_new_data_point())\n",
    "\n",
    "    print(\"ARED COMPLETE\")\n",
    "\n",
    "    # === Matching stats ===\n",
    "    average_o_pt_in_clusters = 0\n",
    "    for cluster in ared.subspace_partition.cluster_list:\n",
    "        average_o_pt_in_clusters += len(cluster.o_pts)\n",
    "    average_o_pt_in_clusters /= len(ared.subspace_partition.cluster_list)\n",
    "    print(average_o_pt_in_clusters)\n",
    "\n",
    "    average_o_pt_in_clusters = 0\n",
    "    clusters_w_o_pts = 0\n",
    "    for cluster in ared.subspace_partition.cluster_list:\n",
    "        if len(cluster.o_pts) != 0:\n",
    "            clusters_w_o_pts += 1\n",
    "            average_o_pt_in_clusters += len(cluster.o_pts)\n",
    "    average_o_pt_in_clusters /= clusters_w_o_pts\n",
    "    print(average_o_pt_in_clusters)\n",
    "\n",
    "    average_l_pt_in_clusters = 0\n",
    "    for cluster in ared.subspace_partition.cluster_list:\n",
    "        average_l_pt_in_clusters += len(cluster.l_pts)\n",
    "    average_l_pt_in_clusters /= len(ared.subspace_partition.cluster_list)\n",
    "    print(average_l_pt_in_clusters)\n",
    "\n",
    "    # total_points = 0\n",
    "    # l_points = 0\n",
    "    # for cluster in ared.subspace_partition.cluster_list:\n",
    "    #     for l_pt in cluster.l_pts:\n",
    "    #         l_points += 1\n",
    "    #         if l_pt + 1 > total_points:\n",
    "    #             total_points = l_pt + 1\n",
    "\n",
    "    #     for o_pt in cluster.o_pts:\n",
    "    #         if o_pt + 1 > total_points:\n",
    "    #             total_points = o_pt + 1\n",
    "\n",
    "    # print(total_points)\n",
    "    # print(l_points)\n",
    "\n",
    "    print(len(ared.labeled_data.data_array))\n",
    "\n",
    "main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "DhYDKdCmhKec",
   "metadata": {
    "id": "DhYDKdCmhKec"
   },
   "source": [
    "#Performance Evaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "G0jbahOYKyms",
   "metadata": {
    "id": "G0jbahOYKyms"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import precision_recall_curve, auc\n",
    "from collections import defaultdict, Counter\n",
    "\n",
    "class PerformanceEvaluator:\n",
    "    def __init__(self, oracle):\n",
    "        self.oracle = oracle\n",
    "        self.predictions = []  # (abs_idx, pred_label, pred_relevance, confidence)\n",
    "        self.ground_truth = {}  # abs_idx -> (true_label, true_relevance)\n",
    "        self.class_discovery = {}  # class -> events_before_discovery\n",
    "        self.total_events = 0\n",
    "        self.total_queries = 0\n",
    "\n",
    "    def record_prediction(self, abs_idx, pred_label, pred_relevance, confidence=1.0):\n",
    "        \"\"\"Record prediction (fast - O(1))\"\"\"\n",
    "        self.predictions.append((abs_idx, pred_label, pred_relevance, confidence))\n",
    "\n",
    "    def record_query(self, abs_idx, true_label, true_relevance):\n",
    "        \"\"\"Record oracle query (fast - O(1))\"\"\"\n",
    "        self.ground_truth[abs_idx] = (true_label, true_relevance)\n",
    "        self.total_queries += 1\n",
    "\n",
    "        # Track class discovery for n_missed metric\n",
    "        if true_label not in self.class_discovery:\n",
    "            self.class_discovery[true_label] = self.total_events\n",
    "\n",
    "    def record_point_processed(self):\n",
    "        \"\"\"Record point processed (fast - O(1))\"\"\"\n",
    "        self.total_events += 1\n",
    "\n",
    "    def get_ground_truth(self, abs_idx):\n",
    "        \"\"\"Get ground truth, query oracle if needed\"\"\"\n",
    "        if abs_idx not in self.ground_truth:\n",
    "            true_label, true_relevance = self.oracle.answer_query(abs_idx)\n",
    "            self.ground_truth[abs_idx] = (true_label, true_relevance)\n",
    "        return self.ground_truth[abs_idx]\n",
    "\n",
    "    def compute_metrics(self):\n",
    "        \"\"\"Compute all metrics at once\"\"\"\n",
    "        if not self.predictions:\n",
    "            return {}\n",
    "\n",
    "        # Get all ground truth\n",
    "        y_true_labels = []\n",
    "        y_pred_labels = []\n",
    "        y_true_relevance = []\n",
    "        y_pred_relevance = []\n",
    "\n",
    "        for abs_idx, pred_label, pred_relevance, _ in self.predictions:\n",
    "            true_label, true_relevance = self.get_ground_truth(abs_idx)\n",
    "            y_true_labels.append(true_label)\n",
    "            y_pred_labels.append(pred_label)\n",
    "            y_true_relevance.append(true_relevance)\n",
    "            y_pred_relevance.append(pred_relevance)\n",
    "\n",
    "        # 1. BALANCED ACCURACY: Average of per-class recalls\n",
    "        class_correct = defaultdict(int)\n",
    "        class_total = defaultdict(int)\n",
    "        for true_label, pred_label in zip(y_true_labels, y_pred_labels):\n",
    "            class_total[true_label] += 1\n",
    "            if true_label == pred_label:\n",
    "                class_correct[true_label] += 1\n",
    "        per_class_recall = {cls: class_correct[cls]/class_total[cls] for cls in class_total}\n",
    "        balanced_accuracy = np.mean(list(per_class_recall.values()))\n",
    "\n",
    "        # 2. CLASS DISCOVERY: Discovery rate + n_missed\n",
    "        all_true_classes = set(y_true_labels)\n",
    "        discovered_classes = set(self.class_discovery.keys())\n",
    "        discovery_rate = len(discovered_classes) / len(all_true_classes)\n",
    "        n_missed = sum(self.class_discovery.values())  # Total events before all discoveries\n",
    "\n",
    "        # 3. RELEVANCE PREDICTION: TP, FP, FN counts\n",
    "        tp = sum(1 for true_rel, pred_rel in zip(y_true_relevance, y_pred_relevance)\n",
    "                if true_rel and pred_rel)\n",
    "        fp = sum(1 for true_rel, pred_rel in zip(y_true_relevance, y_pred_relevance)\n",
    "                if not true_rel and pred_rel)\n",
    "        fn = sum(1 for true_rel, pred_rel in zip(y_true_relevance, y_pred_relevance)\n",
    "                if true_rel and not pred_rel)\n",
    "\n",
    "        rel_precision = tp / max(1, tp + fp)\n",
    "        rel_recall = tp / max(1, tp + fn)  # This is rel_acc from paper\n",
    "        rel_f1 = 2 * rel_precision * rel_recall / max(1, rel_precision + rel_recall)\n",
    "\n",
    "        # 4. RELEVANT CLASS PERFORMANCE: Per-class for relevant classes only\n",
    "        relevant_classes = set()\n",
    "        for i, (true_label, pred_label) in enumerate(zip(y_true_labels, y_pred_labels)):\n",
    "            if y_true_relevance[i]:\n",
    "                relevant_classes.add(true_label)\n",
    "\n",
    "        relevant_class_recalls = []\n",
    "        for cls in relevant_classes:\n",
    "            cls_correct = sum(1 for true_label, pred_label in zip(y_true_labels, y_pred_labels)\n",
    "                            if true_label == cls and pred_label == cls)\n",
    "            cls_total = sum(1 for true_label in y_true_labels if true_label == cls)\n",
    "            cls_recall = cls_correct / max(1, cls_total)\n",
    "            relevant_class_recalls.append(cls_recall)\n",
    "\n",
    "        avg_relevant_recall = np.mean(relevant_class_recalls) if relevant_class_recalls else 0\n",
    "\n",
    "        # 5. QUERY EFFICIENCY\n",
    "        query_rate = self.total_queries / max(1, self.total_events)\n",
    "        relevant_queries = sum(1 for abs_idx in self.ground_truth\n",
    "                             if self.ground_truth[abs_idx][1])\n",
    "        relevant_query_precision = relevant_queries / max(1, self.total_queries)\n",
    "\n",
    "        return {\n",
    "            # GOAL METRICS (most important)\n",
    "            'discovery_rate': discovery_rate,           # Goal 1: Find all classes\n",
    "            'avg_relevant_recall': avg_relevant_recall, # Goal 2: Find all points in relevant classes\n",
    "\n",
    "            # PAPER METRICS (for literature comparison)\n",
    "            'balanced_accuracy': balanced_accuracy,     # Classification across all classes\n",
    "            'n_missed': n_missed,                      # Speed of class discovery\n",
    "            'rel_acc': rel_recall,                     # Relevance prediction recall\n",
    "            'query_rate': query_rate,                  # Query efficiency\n",
    "\n",
    "            # DETAILED METRICS\n",
    "            'rel_precision': rel_precision,\n",
    "            'rel_f1': rel_f1,\n",
    "            'tp': tp, 'fp': fp, 'fn': fn,\n",
    "            'num_relevant_classes': len(relevant_classes),\n",
    "            'relevant_query_precision': relevant_query_precision,\n",
    "            'total_predictions': len(self.predictions),\n",
    "            'total_queries': self.total_queries,\n",
    "            'missed_classes': all_true_classes - discovered_classes\n",
    "        }\n",
    "\n",
    "    def compute_auprc_per_class(self, target_classes=None):\n",
    "        \"\"\"Compute AUPRC for specific classes (optional)\"\"\"\n",
    "        if not self.predictions or not target_classes:\n",
    "            return {}\n",
    "\n",
    "        auprc_scores = {}\n",
    "        for target_class in target_classes:\n",
    "            y_true = []\n",
    "            y_scores = []\n",
    "\n",
    "            for abs_idx, pred_label, _, confidence in self.predictions:\n",
    "                true_label, _ = self.get_ground_truth(abs_idx)\n",
    "                y_true.append(true_label == target_class)\n",
    "                y_scores.append(confidence if pred_label == target_class else 0.0)\n",
    "\n",
    "            if sum(y_true) > 0:\n",
    "                precision, recall, _ = precision_recall_curve(y_true, y_scores)\n",
    "                auprc_scores[target_class] = auc(recall, precision)\n",
    "            else:\n",
    "                auprc_scores[target_class] = 0.0\n",
    "\n",
    "        return auprc_scores\n",
    "\n",
    "    def print_report(self, target_classes=None):\n",
    "        \"\"\"Print concise performance report\"\"\"\n",
    "        metrics = self.compute_metrics()\n",
    "\n",
    "        print(\"A/RED PERFORMANCE REPORT\")\n",
    "        print(\"=\" * 50)\n",
    "\n",
    "        # Goal metrics first\n",
    "        print(f\"GOAL 1 - Class Discovery: {metrics['discovery_rate']:.3f}\")\n",
    "        print(f\"GOAL 2 - Relevant Recall: {metrics['avg_relevant_recall']:.3f}\")\n",
    "\n",
    "        # Paper metrics\n",
    "        print(f\"\\nPAPER METRICS:\")\n",
    "        print(f\"  Balanced Accuracy: {metrics['balanced_accuracy']:.3f}\")\n",
    "        print(f\"  n_missed: {metrics['n_missed']}\")\n",
    "        print(f\"  rel_acc: {metrics['rel_acc']:.3f}\")\n",
    "        print(f\"  query_rate: {metrics['query_rate']:.3f}\")\n",
    "\n",
    "        # Key details\n",
    "        print(f\"\\nDETAILS:\")\n",
    "        print(f\"  Relevance - P:{metrics['rel_precision']:.3f} R:{metrics['rel_recall']:.3f} F1:{metrics['rel_f1']:.3f}\")\n",
    "        print(f\"  TP:{metrics['tp']} FP:{metrics['fp']} FN:{metrics['fn']}\")\n",
    "        print(f\"  Relevant classes: {metrics['num_relevant_classes']}\")\n",
    "        print(f\"  Total predictions: {metrics['total_predictions']}\")\n",
    "\n",
    "        if metrics['missed_classes']:\n",
    "            print(f\"  Missed classes: {sorted(metrics['missed_classes'])}\")\n",
    "\n",
    "        # Optional AUPRC\n",
    "        if target_classes:\n",
    "            auprc = self.compute_auprc_per_class(target_classes)\n",
    "            if auprc:\n",
    "                print(f\"\\nAUPRC per class:\")\n",
    "                for cls, score in sorted(auprc.items()):\n",
    "                    print(f\"  Class {cls}: {score:.3f}\")\n",
    "\n",
    "        print(\"=\" * 50)\n",
    "\n",
    "\n",
    "# Simple integration example\n",
    "def simple_integration_example():\n",
    "    \"\"\"\n",
    "    Simple example of integrating with A/RED.\n",
    "\n",
    "    During streaming:\n",
    "    1. Call evaluator.record_prediction() after each prediction\n",
    "    2. Call evaluator.record_query() when querying oracle\n",
    "    3. Call evaluator.record_point_processed() for each point\n",
    "\n",
    "    After streaming:\n",
    "    4. Call evaluator.print_report() to see results\n",
    "    \"\"\"\n",
    "\n",
    "    # Example usage:\n",
    "    # evaluator = PerformanceEvaluator(oracle)\n",
    "\n",
    "    # During streaming loop:\n",
    "    # evaluator.record_point_processed()\n",
    "    # evaluator.record_prediction(abs_idx, predicted_label, predicted_relevance)\n",
    "    # if query_made:\n",
    "    #     evaluator.record_query(abs_idx, true_label, true_relevance)\n",
    "\n",
    "    # After streaming:\n",
    "    # evaluator.print_report(rare_classes=['8', '9'])\n",
    "\n",
    "    pass"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3.13 (ipykernel)",
   "language": "python",
   "name": "python3.13"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
